\input ../SlidePreamble
\input ../preamble

\newcommand{\solution}[1]{\bigskip {\bf Solution}: #1}

\begin{document}

{\Huge
  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}
  \vfill
  \centerline{\bf Stochastic Gradient Descent (SGD)}
  \vfill
  \centerline{The Classical Convergence Theorem}
  \vfill
  \centerline{Momentum, RMSProp, and Adam}
  \vfill
  \centerline{Decoupling SGD Hyperparameters}

\slide{Vanilla SGD}

$$\Phi \;\minuseq\; \eta \hat{g}$$

\vfill
\begin{eqnarray*}
  \hat{g} & = & E_{(x,y) \sim \mathrm{Batch}}\;\nabla_\Phi\;\mathrm{loss}(\Phi,x,y) \\
  \\
  \\
  g & = & E_{(x,y) \sim \mathrm{Pop}}\;\nabla_\Phi\;\mathrm{loss}(\Phi,x,y) \\
\end{eqnarray*}

\slide{Issues}

\vfill
\begin{itemize}
\item {\bf Gradient Estimation.} The accuracy of $\hat{g}$ as an estimate of $g$.

  \vfill
\item {\bf Gradient Drift (second order structure).} The fact that $g$ changes as the parameters change.

  \vfill
\item {\bf Convergence.} To converge to a local optimum the learning rate must be gradually reduced toward zero.

  \vfill
  \item {\bf Exploration.} Since deep models are non-convex we need to search over the parameter space.  SGD can behave like MCMC.
\end{itemize}

\slide{A One Dimensional Example}

Suppose that $y$ is a scalar, and consider

\begin{eqnarray*}
 \mathrm{loss}(\beta,y) & = & \frac{1}{2}(\beta - y)^2 \\
 \\
  g & = &  \nabla_\beta\;E_{y \sim \mathrm{Pop}}\;\frac{1}{2}(\beta - y)^2 \\
  \\
  & = & \beta - E_{y \sim \mathrm{Pop}} \; y \\
  \\
  \hat{g} & = &\beta - E_{y \sim \mathrm{Batch}} \;y
\end{eqnarray*}

\vfill
Even if $\beta$ is optimal, for a finite batch we will have $\hat{g} \not = 0$.

\slide{The Classical Convergence Theorem}

$$\Phi \;\minuseq \; \eta_t \nabla_\Phi\;\mathrm{loss}(\Phi,x_t,y_t)$$

\vfill
For ``sufficiently smooth'' non-negative loss with

\vfill
$$\eta_t \geq 0\;\;\;\;\;\;\;\;\lim_{t \rightarrow \infty} \;\eta_t = 0\;\;\;\;\;\;\;\;\sum_t \eta_t = \infty \;\;\;\;\;\;\sum_t \;\eta_t^2 < \infty$$

\vfill
we have that the training loss $E_{(x,y) \sim \mathrm{Train}}\; \mathrm{loss}(\Phi,x,t)$ converges to a limit and any limit point of the sequence $\Phi_t$
is a stationary point in the sense that {\huge  $\nabla_\Phi \; E_{(x,y) \sim \mathrm{Train}} \;\mathrm{loss}(\Phi,x,t) = 0$}.

\vfill
{\Large
\vfill
{\bf Rigor Police:} One can construct cases where $\Phi$ diverges to infinity, converges to a saddle point, or even converges to a limit cycle.

}

\slide{Physicist's Proof of the Convergence Theorem}

Since $\lim_{t \rightarrow 0} \;\eta_t = 0$ we will eventually get to arbitrarilly small learning rates.

\vfill
For sufficiently small learning rates any meaningful update of the parameters will be based on an arbitrarily large sample
of gradients at essentially the same parameter value.

\vfill
An arbitrarily large sample will become arbitrarily accurate as an estimate of the full gradient.

\vfill
But since $\sum_t \eta_t = \infty$, no matter how small the learning rate gets, we still can make arbitrarily large motions in parameter space.

\slidetwo{SGD as a form of MCMC}{Learning Rate as a Temperature Parameter}

\centerline{\includegraphics[height= 4in]{../images/AnnealingSGD}}
\centerline{\Large Gao Huang et. al., ICLR 2017}

\slide{Decoupling the Learning Rate $\eta$ from the Batch Size $B$}

Vanilla SGD:


$$\Phi_{t+1} \;\minuseq\; \eta \hat{g}_t$$

\vfill
$$\hat{g}_t = \frac{1}{B} \sum_b \hat{g}_{t,b}$$

\vfill
Where $\hat{g}_{t,b}$ is the gradient of the element $b$ of the batch.

\vfill
Here the effect of $\hat{b}_{t,b}$ on $\Phi$ is $\frac{\eta}{B}\hat{g}_{t,b}$.

\vfill
To make the effect of $\hat{g}_{t,b}$ independent of $B$ we can use
{\color{red} $$\eta = B\eta_0$$}
where $\eta_0$ is optimal for vanilla SGD with $B = 1$.

\slide{Decoupling $\eta$ from $B$}

Recent work has show that using $\eta = B\eta_0$ leads to effective learning with very large (highly parallel)
batches.

\vfill
{\bf Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}, Goyal et al., 2017.

\slideplain{Momentum}

\begin{eqnarray*}
  {\color{red} v_t} & {\color{red} =} & {\color{red} \mu v_{t-1} + \eta * \hat{g}_t} \;\;\;\mbox{$\mu$ is typically .9 or .99}\\
  \\
  {\color{red} \Phi_{t+1}} & {\color{red} =} & {\color{red} \Phi_t -  v_t} \\
\end{eqnarray*}

The theory of momentum is generally given in terms of second order structure and total gradients (GD rather than SGD).


\centerline{\includegraphics[width = 4in]{../images/momentum}}
\centerline{\Large Rudin's blog}


\slideplain{Momentum}

GD analyses seem unlikely to apply to SGD.

\vfill
Second order analyses are of questionable value for SDG in very large dimension.

\vfill
But momentum is widely used in SGD.

\vfill
To better understand momentum we will rewrite it as a running average.

\slide{Running Averages}
Consider a sequence $x_1$, $x_2$, $x_3$, $\ldots$.
\vfill
For $t \geq N$, consider the average of the $N$ most recent values.
$$\overline{x}_t = \frac{1}{N} \sum_{\tilde{t} = t-N+1}^t x_{\tilde{t}}$$

\vfill
This can be approximated efficiently with
\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\slide{Deep Learning Convention for Running Averages}

In deep learning a running average

$$\tilde{x}_t = \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t$$

\vfill
is written as

$$\tilde{x}_t = \beta\tilde{x}_{t-1} + (1-\beta)x_t$$

\vfill
where
$$\beta = 1 - 1/N$$

\vfill
Typical values for $\beta$ are .9, .99 or .999 corresponding to $N$ being 10, 100 or 1000.

\vfill
It will be convenient here to use $N$ rather than $\beta$.

\slide{Momentum as a Running Average}

\begin{eqnarray*}
v_t & = & \mu v_{t-1} + {\color{red} \eta \hat{g}_t} \\
\\
& = & \left(1-\frac{1}{N_g}\right) v_{t-1} + {\color{red} \frac{1}{N_g}(N_g\eta \hat{g}_t)}
\end{eqnarray*}

\vfill
Here we rewrite this in terms of a direct running average of the gradient.

$$\tilde{g}_t = \left(1-\frac{1}{N_g}\right)\tilde{g}_{t-1} + \frac{1}{N_g} \hat{g}_t$$

\vfill
Since averaging is a linear operation, we get

$$v_t = N_g \eta \tilde{g}_t$$

\slideplain{Momentum as a Running Average}

We have now shown that the momentum update is

$$\Phi_{t+1}  =  \Phi_t - v_t  = \Phi_t - N_g\eta\tilde{g}_t$$

\vfill
Hence momentum can be written as

\begin{eqnarray*}
\tilde{g}_t & = & \left(1-\frac{1}{N_g}\right)\tilde{g}_{t-1} + \frac{1}{N_g} \hat{g}_t \\
\\
\Phi_{t+1} & = &  \Phi_t - \tilde{\eta}\tilde{g}_t \\
\\
\tilde{\eta} & = & N_g\eta
\end{eqnarray*}

\slideplain{Generalizing $\eta = B\eta_0$}

We will use the general principle that the effect of a single batch element $\hat{g}_{t,b}$ on the model should be $\eta_0\hat{g}_{t,b}$ where $\eta_0$ is the optimal learning rate
for Vanilla SGD with batch size 1.

\vfill
This is consisting with the empirically validated scaling of $\eta$ with batch size.

\vfill
It is also intuitively motivated by the idea that the model is changing slowly and delaying the effect of $\hat{g}_{t,b}$ does not change the total effect it should have.

\slide{Decoupling $\eta$ from $N_g$}

\begin{eqnarray*}
\tilde{g}_t & = & \left(1-\frac{1}{N_g}\right)\tilde{g}_{t-1} + \frac{1}{N_g} \hat{g}_t \\
\\
\Phi_{t+1} & = &  \Phi_t - \tilde{\eta}\tilde{g}_t
\end{eqnarray*}

For $\Delta t >> N_g$, the effect of $\hat{g}_t$ on $\Phi_{t+\Delta t}$ is approximately

$$\frac{\tilde{\eta}\hat{g}_t}{N_g}\sum_{i = 0}^\infty \;\left(1 - \frac{1}{N_g}\right)^i = \frac{\tilde{\eta}\hat{g}_t}{N_g}\;N_g = \tilde{\eta}\hat{g}_t$$

\vfill
{\color{red} So the effect of $\hat{g}_t$ on the model is $\tilde{\eta}\hat{g}_t$ independent of $N_g$.}

\vfill
This suggests that the optimal value of $\tilde{\eta}$ is independent of $N_g$.

\slide{Decoupling $\eta$ from $N_g$}

Using the same value of $\tilde{\eta}$ independent of $N_g$ gives

$$\tilde{\eta} = B\eta_0 = N_g\eta$$

where, as before, $\eta_0$ is the optimal learning rate for Vanilla SGD with $B = 1$.

\vfill
Solving for $\eta$ we get
{\color{red} $$\eta = \frac{B\eta_0}{N_g}$$}

\vfill
{\bf Don't Decay the Learning Rate, Increase the Batch Size}, Smith et al., 2018

\slide{Decoupling $\eta$ from $N_g$}

{\color{red} $$\eta = \frac{B\eta_0}{N_g}$$}


\vfill
One should set $\beta$ to be large enough so as to saturate the hardware (your GPUs should be running at maximum FLOPs).

\vfill
The optimnal value of $\eta_0$ should be fairly insensitive to the choice of $B$ and $N_g$.

\slide{Decoupling $N_g$ from $B$}

Let $N^0_g$ be a momentum parameter appropriate for $B=1$.

\vfill
Momentum is based on a moving average of gradients

$$\tilde{g}_{t+1} = \left(1-\frac{1}{N_g}\right)\tilde{g}_t + \frac{1}{N_g}\hat{g}_t$$

In the presence of minibatching this is (approximately) averaging over $N_g$ minibatches.

\vfill
But to average over $N^0_g$ individual gradients we should use

{\color{red} $$N_g = \frac{N^0_g}{B}$$}

\slide{Putting It Together}

\begin{eqnarray*}
N_g & = & \min\left(1,N^0_g/B\right) \\
\\
\eta & = & \frac{B\eta_0}{N_g}
\end{eqnarray*}

We can take $B$ to be as large as memory allows and then tune $N^0_g$ and $\eta_0$ rather than $N_g$ and $\eta$.

\vfill
The optimal values for $N^0_g$ and $\eta_0$ should be relatively independent of the batch size $B$.

\slide{RMSProp and Adam}

RMSProp and Adam are ``adaptive'' SGD methods --- the effective learning rate is computed from statistics of the data rather than set as a fixed hyper-parameter (although
the adaptation algorithm itself still has hyper-parameters).

\vfill
Different effective learning rates are used for different model parameters.

\vfill
Adam is typically used in NLP while Vanilla SGD is typically used in vision.  This may be related to the fact that batch normalization is used in vision but not in NLP.

\slide{RMSProp}

RMSProp is based on a running average of $\hat{g}[i]^2$ for each scalar model parameter $i$.

\begin{eqnarray*}
{\color{red} s_t[i]} & {\color{red} =} & {\color{red} \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2}\;\;\;\mbox{$N_s$ typically 100 or 1000} \\
\\
{\color{red} \Phi_{t+1}[i]} & {\color{red} =} & {\color{red} \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i]} + \epsilon}\;\; \hat{g}_t[i]}
\end{eqnarray*}

\slide{RMSProp}
The second moment of a scalar random variable $x$ is $E\;x^2$

\vfill
The variance $\sigma^2$ of $x$ is $E \;(x - \mu)^2$ with $\mu = E\;x$.

\vfill
RMSProp uses an estimate $s[i]$ of the second moment of the random scalar $\hat{g}[i]$.

\vfill
For $g[i]$ small $s[i]$ approximates the variance of $\hat{g}[i]$.

\vfill
There is a ``centering'' option in PyTorch RMSProp that switches from the second moment to the variance.

\slide{RMSProp Motivation}
\begin{eqnarray*}
{\color{red} \Phi_{t+1}[i]} & {\color{red} =} & {\color{red} \Phi_t[i] - \frac{\eta}{\sqrt{s_t[i]} + \epsilon}\;\; \hat{g}_t[i]}
\end{eqnarray*}

\vfill
One interpretation of RMSProp is that a low variance gradient has less statistical uncertainty and hence needs less averaging before making the update.

\slide{RMSProp is Theoretically Mysterious}

{\color{red} $$\Phi[i] \;\minuseq \; \eta\;\frac{\hat{g}[i]}{\sigma[i]}\;\;(1)\hspace{5em}\Phi[i] \;\minuseq \; \eta\;\frac{\hat{g}[i]}{\sigma^2[i]}\;\;(2)$$}

\vfill
Although (1) seems to work better, (2) is better motivated theoretically.  To see this we can consider units.

\vfill
If parameters have units of ``weight'', and loss is in bits, then (2) type checks with $\eta$ having units of inverse bits --- the numerical value of $\eta$
has no dependence on the choice of the weight unit.

\vfill
Consistent with the dimensional analysis, many theoretical analyses support (2) over (1) contrary to apparent empirical performance.
\slide{Adam --- Adaptive Momentum}

Adam combines momentum and RMSProp (although PyTorch RMSProp also supports momentum).

\vfill
Adam also uses ``bias correction'' which probably accounts for it's popularity over RMSProp in practice.

\slide{Bias Correction}

Consider a standard moving average.

\begin{eqnarray*}
\tilde{x}_0 & = & 0 \\
\\
\tilde{x}_t & = & \left(1-\frac{1}{N}\right)\tilde{x}_{t-1} + \left(\frac{1}{N}\right)x_t
\end{eqnarray*}

\vfill
For $t < N$ the average $\tilde{x}_t$ will be strongly biased toward zero.

\slide{Bias Correction}

The following running average maintains the invariant that $\tilde{x}_t$ is exactly the average of $x_1,\ldots,x_t$.

\begin{eqnarray*}
\tilde{x}_t & = & \left(\frac{t-1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t \\
\\
\\
& = & \left(1-\frac{1}{t}\right)\tilde{x}_{t-1} + \left(\frac{1}{t}\right)x_t
\end{eqnarray*}

\vfill
We now have $\tilde{x}_1 = x_1$ independent of any $x_0$.

\vfill
But this fails to track a moving average for $t >> N$.

\slide{Bias Correction}

The following avoids the initial bias toward zero while still tracking a moving average.

\begin{eqnarray*}
\tilde{x}_t & = & \left(1-\frac{1}{\min(N,t)}\right)\tilde{x}_{t-1} + \left(\frac{1}{\min(N,t)}\right)x_t
\end{eqnarray*}

\vfill
The published version of Adam has a more obscure form of bias correction which yields essentially the same effect.

\slide{Adam (simplified)}

\begin{eqnarray*}
  \tilde{g}_{t}[i] & = & \left(1-\frac{1}{\min(t,N_g)}\right)\tilde{g}_{t-1}[i] + \frac{1}{\min(t,N_g)} \hat{g}_t[i] \\
  \\
  \\
  s_{t}[i] & = & \left(1-\frac{1}{\min(t,N_s)}\right)s_{t-1}[i] + \frac{1}{\min(t,N_s)} \hat{g}_t[i]^2 \\
  \\
  \\
\Phi_{t+1}[i] & =  & \Phi_t - \frac{\eta}{\sqrt{s_{t}[i]} + \epsilon}\;\;\tilde{g}_{t}[i]
\end{eqnarray*}

\slide{Decoupling Hyperparametera}

The following reparameterization should be helpful for Adam.

\begin{eqnarray*}
N_g & = & min(1,N^0_g/B) \\
\\
\eta & = & \epsilon B \eta_0
\end{eqnarray*}

\vfill
Empirically, tuning $\epsilon$ is important.

\vfill
Some coupling remains between $\eta_0$ and $\epsilon$.

\vfill
$N_s$ should also be adapted to $B$ but this is problematic --- see the next slide.

\slide{Making $s_t[i]$ batch size invariant}

rather than
$$s_t[i] = \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \hat{g}_t[i]^2$$
\vfill
we would like
\begin{eqnarray*}
s_t[i] & = &  \left(1-\frac{1}{N_s}\right) s_{t-1}[i] + \frac{1}{N_s} \left(\frac{1}{B} \sum_b \hat{g}_{t,b}[i]^2\right) \\
\\
\\
N_s & = & \min\left(1,N^0_s/B\right)\;\;\;\mbox{$N^0_s$ optimal for $B=1$}
\end{eqnarray*}

\slide{Making $s_t[i]$ Batch Size Invariant}

In PyTorch this is difficult because ``optimizers'' are defined as a function of $\hat{g}_t$.

\vfill
$\hat{g}_t[i]$ is not sufficient for computing $\sum_b \;\hat{g}_{t,b}[i]^2$.


\vfill
To compute $\sum_b \;\hat{g}_{t,b}[i]^2$ we need to modify the backward method of all PyTorch objects!

\slide{Summary}

We have considered Vanilla SGD, Momentum, RMSProp and Adam.

\vfill
Vanilla tends to be used in vision while Adam tends to be used in NLP.

\vfill
Reparameterization of the PyTorch hyperparameters can decouple hyperparameters simplifying hyperparameter search.


\slide{END}

} \end{document}

