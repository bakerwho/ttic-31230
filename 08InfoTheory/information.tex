\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}

  \vfill
  \centerline{\bf Shannon's Source Coding Theorem}
  \vfill
  \centerline{\bf Huffman Coding}
  \vfill
  \centerline{\bf Perils of Continuous Information Theory}
  \vfill
\vfill
\vfill

\slide{Source Coding Theorem}

Consider a probability distribution $\mathrm{Pop}$ on a finite set $S$.

\vfill
Consider a code $C$ assigning a bit string code word $C(y_1,\ldots,y_B)$ to each possible batch of $B$ elements with $y_i \sim \mathrm{Pop}$.

\vfill
Source coding theorem: As $B \rightarrow \infty$ the optimal coding uses exactly $H(\mathrm{Pop})$
bits per batch element.

\slide{Prefix Free Codes}

Let $S$ be a finite set.

\vfill
Let $C$ be assignment of a bit string $C(y)$ to each $y \in S$.

\vfill
$C$ is called {\em prefix-free} if for $x \not = y$ we have that $C(x)$ is not a prefix of $C(y)$.

\vfill
A concatenation of sequence of prefix-free code words can be uniquely segmented (parsed) back into a sequence of code words.

\slide{Prefix-Free Codes as Trees and as Probabilities}

A prefix-free code defines a binary branching tree --- branch on the first code bit, then the second, and so on.

\vfill
The leaves of this tree are labeled with the elements of $S$.

\vfill
The code defines a probability distribution on $S$ by randomly selecting branches.

\vfill
We have $P_C(y) = 2^{-|C(y)|}$.

\slide{The Source Coding Theorem}

(1) There exists a prefix-free code $C$ such that
$$|C(y)| <= (- \log_2 \mathrm{Pop}(y)) + 1$$
and hence
$$E_{y\sim \mathrm{Pop}} |C(y)| \leq H(\mathrm{Pop}) +1$$

\vfill
(2) For any prefix-free code $C$

$$E_{y \sim \mathrm{Pop}}\;|C(y)| = E_{y \sim \pop} -\ln \;P_C(y) = H(\pop,P_C) \geq H(\mathrm{Pop})$$

\slide{Code Construction}

\vfill
We construct a code by iterating over $y \in S$ in order of decreasing probability (most likely first).

\vfill
For each $y$ select a code word $C(y)$ (a tree leaf) with length (depth)

\vfill
$$|C(y)| = \lceil - \log_2 \mathrm{Pop}(y)\rceil$$

\vfill
and where $C(y)$ is not an extension of (under) any previously selected code word.

\slide{Code Existence Proof}

At any point before coding all elements of $S$ we have
$$\sum_{y \in \mathrm{Defined}} 2^{-|C(y)|} \leq \sum_{y \in \mathrm{Defined}} \mathrm{Pop}(y) < 1$$

\vfill
Therefore there exists an infinite descent into the tree that misses all previous code words.

\vfill
Hence there exists a code word $C(x)$ not under any previous code word with
$|C(x)| = \lceil - \log_2 \mathrm{Pop}(y)\rceil$.

\vfill
Furthermore $C(x)$ is at least as long as all previous code words and hence $C(x)$ is not a prefix of any previously selected code word.

\slide{Huffman Coding}

Maintain a list of trees $T_1,\;\dots,\;T_N$.

\vfill
Inititally each tree is just one root node labeled with an element of $S$.

\vfill
Each tree $T_i$ has a weight equal to the sum of the probabilities of the nodes on the leaves of that tree.

\vfill
Repeatedly merge the two trees of lowest weight into a single tree until all trees are merged.

\slide{Optimality of Huffman Coding}

{\bf Theorem}: The Huffman code $T$ for $\mathrm{Pop}$ is optimal --- for any other tree $T'$ we have $d(T;\mathrm{Pop}) \leq d(T';\mathrm{Pop})$.

\vfill
{\bf Proof}: The algorithm maintains the invariant that there exists an optimal tree including
all the subtrees on the list.

\vfill
To prove that a merge operation maintains this invariant we consider any tree containing the given subtrees.

\vfill
Consider the two subtrees $T_i$ and $T_j$ of minimal weight.  Without loss of generality we can assume that $T_i$ is at least as deep as $T_j$.

\vfill
Swapping the sibling of $T_i$ for $T_j$ brings $T_i$ and $T_j$ together and can only improve the average depth.

\slide{Optimality of Huffman Coding}

Why the swap operation cannot increase entropy.

\vfill
Let $d(n)$ be the depth of node $n$, let $P(n)$ be $2^-d(n)$, and let $W(n)$ be the weight of $n$.

\vfill
For $P(n_1) \geq P(n_2)$ and $W_1 \geq W_2$ then

\vfill
$$P(n_1)W_1 + P(n_2)W_2 \geq P(n_1)W_2 + P(n_2)W_1$$

\slide{Perils of Differential Entropy}

Consider a continuous density $p(x)$.  For example

\vfill
$$p(x) = \frac{1}{\sqrt{2\pi}\; \sigma}\; e^{\frac{-x^2}{2\sigma^2}}$$

\vfill
Differential entropy is often defined as

\vfill
$$H(p) \doteq \int \left(\ln \frac{1}{p(x)}\right) p(x) dx$$

\slide{Finite Differential Entropy is Not Meaningful}

\begin{eqnarray*}
  H({\cal N}(0,\sigma)) & = &  + \int \left( \ln(\sqrt{2\pi} \sigma) + \frac{x^2}{2\sigma^2}\right) p(x) dx \\
  \\
  & = & \ln(\sigma) + \ln(\sqrt{2\pi}) + \frac{1}{2}
\end{eqnarray*}

\vfill
But if we take $y \doteq x/2$ we get $H(y) = H(x) - \ln 2$.

\vfill
Also for $\sigma << 1$, we get $H(p) < 0$

\vfill
Hence differential entropy then depends on the choice of units --- a distributions on lengths will have a different entropy
when measuring in inches than when measuring in feet.

\slide{Differential Entropy is Always Infinite}

Consider quantizing the the real numbers into bins.

\vfill
A continuous probability densisty $p$ assigns a probability $p(B)$ to each bin.

\vfill
As the bin size decreases toward zero the entropy of the bin distribution increases toward $\infty$.

\vfill
A meaningful convention is that $H(p) = +\infty$ for any continuous density $p$.

\slide{Differential KL-divergence is Meaningful}

$$KL(p,q) = \int \left( \ln \frac{p(x)}{q(x)}\right) p(x) dx$$

\vfill
This integral can be computed by dividing the real numbers into bins and computing the $KL$ divergence between the distributions on bins.

\vfill
The KL divergence between the bin distribution typically approaches a finite limit as the bin size goes to zero.

\slide{KL-Divergence can also be Infinite}

$$KL(p,q) = E_{x \sim p}\; \log\frac{p(x)}{q(x)}$$
\vfill
In either the discrete or continuous case, if a set is assigned nonzero probability by $p$ but zero probability by $q$ then $KL(p,q) = +\infty$.

\vfill
If every set assigned nonzero probability by $p$ is also assigned nonzero probability by $q$ then we say that $p$ is absolutely continuous with respect to $q$.

\slide{Random Variables}

We consider variables where a single draw form the population determines a value for each variable.

\vfill
This is the formal definition of a ``random variable''.

\vfill
Each random variable has a probability distribution defined by the distribution on the population.

\vfill
We write $H(x)$ for the entropy of the distribution on $x$.

\slide{Mutual Information}

For two random variables $x$ and $y$ there is a distribution on pairs $(x,y)$ determined by the population distribution.

\vfill
Mutual information concerns the relationship between the distribution on $(x,y)$ and the marginal distributions on $x$ and $y$.

\vfill
For the discrete case we can write.

\vfill
\begin{eqnarray*}
  I(x,y) & \doteq & H(x) + H(y) - H(x,y)
\end{eqnarray*}

\vfill
This can be viewed as a quantity of non-independence --- independent variables have zero mutual information.

\slide{Conditional Entropy}

For the discrete case conditional entropy $H(y|x)$ is defined by

\begin{eqnarray*}
  H(y|x) & \doteq & \sum_x \mathrm{Pop}(x) \sum_y \mathrm{Pop}(y|x) - \log \mathrm{Pop}(y|x) \\
  \\
  & = & E_{x\sim \mathrm{Pop}}\;E_{y \sim \mathrm{Pop}|x}\;- \log \mathrm{Pop}(y|x) \\
  \\
  & = & E_{x\sim \mathrm{Pop}}\;H(\mathrm{Pop}(y|x))
\end{eqnarray*}

\slide{More Identities}
\vfill
For the discrete case we have.

\vfill
\begin{eqnarray*}
  I(x,y) & = & H(x) - H(x|y) \\
  \\
  & = & H(y) - H(y|x) \\
  \\
  & = & KL(\mathrm{Pop}(x,y),\mathrm{Pop}(x)\times\mathrm{Pop}(y))
\end{eqnarray*}

The last identity can be taken as a definition of $I(x,y)$ in the continuous case.

\slide{END}

}
\end{document}
